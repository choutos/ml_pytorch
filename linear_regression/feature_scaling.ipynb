{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "What is feature scaling? Feature scaling is a way of transforming your data into a common rage of values. There are two common scalings:\n",
    "    1. Standardizing\n",
    "    2. Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing\n",
    "**Standardizing** is completed by taking each value of your column, subtracting the mean of the column, and then dividing by the standard deviation of the column. In Python, let's say you have a column in `df`called `height`. You could create a standardized height as:\n",
    "```\n",
    "df[\"height_standard\"] = (df[\"height\"] - df[\"height\"].mean()) / df[\"height\"].std()\n",
    "```\n",
    "This will create a new \"standardized\" column where each value is a comparison to the mean of the column, and a new, standardized value can be interpreted as the number of standard deviations the original height was from the mean. This type of feature scaling is by far the most common of all techniques (for the reasons discussed here, but also likely because of precedent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing\n",
    "\n",
    "A second type of feature scaling that is very popular is known as **normalizing**. With normalizing, data are scaled between 0 and 1. Using the same example as above, we could perform normalizing in Python in the following way:\n",
    "```\n",
    "df[\"height_normal\"] = (df[\"height\"] - df[\"height\"].min()) /     \\\n",
    "                      (df[\"height\"].max() - df['height'].min())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Should I Use Feature Scaling?\n",
    "In may machine learning algorithms, the result will change depending on the units of your data. This is especially true in two specific cases:\n",
    "1. When you algorithm uses a distance-based metric to predict.\n",
    "2. When you incorporate regularization.\n",
    "\n",
    "## Distance Based Metrics\n",
    "Later, we will see one common supervised learning technique that is based on the distance points from one another called [Support Vector Machines (or SVMs)](https://en.wikipedia.org/wiki/Support_vector_machine). Another technique that involves distance based methods to determine a prediction is [k-nearest neighbors (or k-nn)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With either of these techniques, choosing not to scale your data may lead to drastically different (and likely misleading) ending predictions.\n",
    "\n",
    "For this reason, choosing some sort of feature scaling is necessary with these distance based techniques.\n",
    "\n",
    "## Regularization\n",
    "When you start introducing regularization, you will again want to scale the features of your model. The penalty on particular coefficients in regularized linear regression techniques depends largely on the scale associated with the features. When one feature is on a small range, say from 0 to 10, and another is on a large range, say from 0 to 1.000.000, applying regularization is going to unfairly punish the feature with the small range. Features with small ranges need to have larger coefficients compared to features with large ranges in order to have the same effet on the outcome of the data. (Think about how _ab = ba_ for two numbers _a_ and _b_.) Therefore, if regularization could remove one of those two features with the smae net increase in error, it would rather remove the small-ranged feature with the large coefficient, since that would reduce the regularization term the most.\n",
    "\n",
    "Again, this means you will want to scale features any time you are appying regularization.\n",
    "* A [useful post](https://www.quora.com/Why-do-we-normalize-the-data) on the importance of feature scaling when using regularization.\n",
    "\n",
    "A point raised in the article above is that feature scaling can speed up convergence of your machine learning algorithms, which is an important consideration when you scale machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling Exercise\n",
    "Previously, we saw how regularization removes features from a model (by setting their coefficients to zero) if the penalty for removing them is small. In this exercise, we'll revisit the same dataset as before and see how scaling the features changes which features are favored in a regularization step.\n",
    "\n",
    "See the \"Regularization\" notebook for more details. The only thing different for this exercise compared to the previous one is the addition of a new step after loading the data, where you will use sklearn's **[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)** to standardize the data before fitting a linear regression model to the data with L1 (Lasso) regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the data\n",
    "* The data is in the file 'data/regular.csv'. Note that **there is no header row** on this file.\n",
    "* Split the data so that the six predictor features (first six columns) are stored in `X`, and the outcome feature (last column) is stored in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/regular.csv', header=None)\n",
    "X = train_data.iloc[:,:-1]\n",
    "y = train_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform feature scaling on data via standardization   <--- (NEW)\n",
    "Create an instance of sklearn's **[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)** and assign it to the variable `scaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the standardization scaling object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the scaling parameters by using the `.fit_transform()` method on the predictor feature array, which also returns the predictor variables in their standardized values. Store those standardized values in `X_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the standardization parameters and scale the data.\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fit data using linear regression with Lasso regularization\n",
    "Create an instance of sklearn's **[Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)** class and assign it to the variable `lasso_reg`. You don't need to set any parameter values: use the default values for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear regression model with lasso regularization.\n",
    "lasso_reg = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Lasso` object's `.fit()` method to fit the regression model onto the data. Make sure that you apply the fit to the _standardized_ data from the previous step `(X_scaled)`, not the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model.\n",
    "lasso_reg.fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Inspect the coefficients of the regression model\n",
    "Obtain the coefficients of the fit regression model using the `.coef_` attribute of the `Lasso` object. Store this in the `reg_coef` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           3.90753617   9.02575748  -0.         -11.78303187\n",
      "   0.45340137]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print out the coefficients from the regression model.\n",
    "reg_coef = lasso_reg.coef_\n",
    "print(reg_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
